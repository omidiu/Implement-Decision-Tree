{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    ['Pointy', 'Round',     'Present', 1],\n",
    "    ['Floppy', 'Not round', 'Present', 1],\n",
    "    ['Floppy', 'Round',     'Absent',  0],\n",
    "    ['Pointy', 'Not round', 'Present', 0],\n",
    "    ['Pointy', 'Round',     'Present', 1],\n",
    "    ['Pointy', 'Round',     'Absent',  1],\n",
    "    ['Floppy', 'Not round', 'Absent',  0],\n",
    "    ['Pointy', 'Round',     'Absent',  1],\n",
    "    ['Floppy', 'Round',     'Absent',  0],\n",
    "    ['Floppy', 'Round',     'Absent',  0],\n",
    "])\n",
    "\n",
    "X, Y = data[:, :-1], data[:, -1]\n",
    "Y = Y.reshape((-1, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray, default=None\n",
    "        The dataset includes X and Y\n",
    "    children: dict(feat_value: Node), default=None\n",
    "        Dict of children\n",
    "    split_on: int, default=None\n",
    "        Index of the feature that node was split on that\n",
    "    pred_class : str, default=None\n",
    "        The predicted class for the node (only applicable to leaf nodes)\n",
    "    is_leaf: bool, default=False\n",
    "        Determine whether the node is leaf or not\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> feat_index = 0     # Ear Shape\n",
    "    >>> root = Node(data=all_data, split_on=feat_index)\n",
    "    >>> pointy_node = Node(data=pointy_data, is_leaf=True)\n",
    "    >>> floppy_node = Node(data=floppy_data, is_leaf=True)\n",
    "    >>> root.children = {\"Pointy\": pointy_node, \"Floppy\": floppy_node}\n",
    "\n",
    "    Visualization\n",
    "    -------------\n",
    "                                 root  (data = all_data, split_on = 0, is_leaf=False)\n",
    "                                /    \\\n",
    "                               /      \\\n",
    "                              /        \\\n",
    "                             /          \\\n",
    "                     pointy_node     floppy_node\n",
    "    (data=pointy_data, is_leaf=True)    (data=floppy_data, is_leaf=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False):\n",
    "\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = is_leaf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree model to the provided dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.ndarray\n",
    "            The input features of the dataset.\n",
    "\n",
    "        Y: numpy.ndarray\n",
    "            The target labels of the dataset.\n",
    "        \"\"\"\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "\n",
    "\n",
    "    def meet_criteria(self, node):\n",
    "        \"\"\"\n",
    "        Check if the criteria for stopping the tree expansion is met for a given node. Here we only check if the entropy of the target values (y) is zero.\n",
    "        Additionally, you can customize criteria based on your specific requirements. For instance, you can set the maximum depth for the decision tree or incorporate other conditions for stopping the tree expansion. Modify the implementation of this method according to your desired criteria.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        node : Node\n",
    "            The node to check for meeting the stopping criteria.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        bool\n",
    "            True if the criteria is met, False otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 else False\n",
    "\n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        \"\"\"\n",
    "        Get the target (y) from the data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            The input data containing features and the target variable.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        y: numpy.ndarray\n",
    "            The target variable extracted from the data.\n",
    "\n",
    "        \"\"\"\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_entropy(Y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y: numpy.ndarray\n",
    "            The labels array.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        entropy: flaot\n",
    "            The entropy value of the given labels.\n",
    "\n",
    "        Examples:\n",
    "        ----------\n",
    "        >>> Y_1 = np.array([[1], [1], [0], [0]])\n",
    "        >>> DecisionTreeClassifier.calculate_entropy(Y_1)\n",
    "        1.0\n",
    "        >>> Y_2 = np.array([[1], [1], [1], [1]])\n",
    "        >>> DecisionTreeClassifier.calculate_entropy(Y_2)\n",
    "        0.0\n",
    "        \"\"\"\n",
    "        _, labels_counts = np.unique(Y, return_counts=True)\n",
    "        total_instances = len(Y)\n",
    "        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n",
    "        return entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        \"\"\"\n",
    "        Get the predicted class label based on the majority.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y : numpy.ndarray\n",
    "            The array of class labels.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        str\n",
    "            The predicted class label.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "\n",
    "    def best_split(self, node):\n",
    "        \"\"\"\n",
    "        Find the best split for the given node.\n",
    "        (data in node.data)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        node: Node\n",
    "            The node for which the best split is being determined.\n",
    "\n",
    "        If the node meets the criteria to stop splitting:\n",
    "            - Mark the node as a leaf.\n",
    "            - Assign a predicted class for future predictions based on the target values (y).\n",
    "            - return.\n",
    "\n",
    "        Otherwise:\n",
    "            - Initialize variables for tracking the best split.\n",
    "            - Iterate over the features to find the best split.\n",
    "            - Split the data based on each feature and calculate the weighted entropy of the split.\n",
    "            - Compare the current weighted entropy with the previous best entropy.\n",
    "            - Update the best split variables if the current split has lower entropy.\n",
    "            - update the node with the best split information, including child nodes and the feature index used for the split.\n",
    "            - Recursively call the best_split function for each child node.\n",
    "\n",
    "        \"\"\"\n",
    "        # Check if the node meets the criteria to stop splitting\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "\n",
    "        # Initialize variables for tracking the best split\n",
    "        index_feature_split = -1\n",
    "        min_entropy = 1\n",
    "\n",
    "        # iterate over all features, ignore (y)\n",
    "        for i in range(data.shape[1] - 1):\n",
    "            split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n",
    "            if weighted_entropy < min_entropy:\n",
    "                child_nodes, min_entropy = split_nodes, weighted_entropy\n",
    "                index_feature_split = i\n",
    "\n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "\n",
    "        # Recursively call the best_split function for each child node\n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "\n",
    "\n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        \"\"\"\n",
    "        Split the dataset based on a specific feature index.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: numpy.ndarray\n",
    "            The dataset to be split.\n",
    "\n",
    "        feat_index: int\n",
    "            The index of the feature to perform the split.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        split_nodes: dict\n",
    "            A dictionary of split nodes.\n",
    "            (feature value as key, corresponding node as value)\n",
    "\n",
    "        weighted_entropy: float\n",
    "            The weighted entropy of the split.\n",
    "        \"\"\"\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "        total_instances = len(data)\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            node = Node(data=partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = self.get_y(partition)\n",
    "            node_entropy = self.calculate_entropy(partition_y)\n",
    "            weighted_entropy += (len(partition) / total_instances) * node_entropy\n",
    "\n",
    "        return split_nodes, weighted_entropy\n",
    "\n",
    "\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Recursively traverse the decision tree to predict the class label for a given input.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x:\n",
    "            The input for which to make a prediction.\n",
    "\n",
    "        node:\n",
    "            The current node being traversed in the decision tree.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        predicted_class:\n",
    "            The predicted class label for the input feature.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the current node is a leaf node\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "\n",
    "        # Get the feature value at the split point for the current node\n",
    "        feat_value = x[node.split_on]\n",
    "\n",
    "        # Recursively traverse the decision tree using the child node corresponding to the feature value\n",
    "        predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "\n",
    "        return predicted_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given input features.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.ndarray\n",
    "            The input features for which to make predictions. Should be a 2D array-like object.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        predictions: numpy.ndarray\n",
    "            An array of predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Traverse the decision tree for each input and make predictions\n",
    "        predictions = np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "        return predictions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['1', '1', '0'], dtype='<U1')"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, Y)\n",
    "model.predict([\n",
    "    ['Pointy', 'Round',     'Present'],\n",
    "    ['Floppy', 'Not round', 'Present'],\n",
    "    ['Floppy', 'Round',     'Absent']\n",
    "]) # ['1' '1' '0']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root\n",
      "- split on: Ear Shape\n",
      "- children: {'Floppy': <__main__.Node object at 0x7f87d92d09d0>, 'Pointy': <__main__.Node object at 0x7f87e89c8d30>} \n",
      "\n",
      "Pointy Node\n",
      "- split on: Face Shape\n",
      "- children: {'Not round': <__main__.Node object at 0x7f87d9ba5820>, 'Round': <__main__.Node object at 0x7f87d9ba5160>}\n",
      "- data: \n",
      " [['Pointy' 'Round' 'Present' '1']\n",
      " ['Pointy' 'Not round' 'Present' '0']\n",
      " ['Pointy' 'Round' 'Present' '1']\n",
      " ['Pointy' 'Round' 'Absent' '1']\n",
      " ['Pointy' 'Round' 'Absent' '1']] \n",
      "\n",
      "Floppy Node\n",
      "- split on: Whiskers\n",
      "- children: {'Absent': <__main__.Node object at 0x7f87e89c8460>, 'Present': <__main__.Node object at 0x7f87d9ba50a0>}\n",
      "- data: \n",
      " [['Floppy' 'Not round' 'Present' '1']\n",
      " ['Floppy' 'Round' 'Absent' '0']\n",
      " ['Floppy' 'Not round' 'Absent' '0']\n",
      " ['Floppy' 'Round' 'Absent' '0']\n",
      " ['Floppy' 'Round' 'Absent' '0']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_to_name = {\n",
    "    0: 'Ear Shape',\n",
    "    1: 'Face Shape',\n",
    "    2: 'Whiskers'\n",
    "}\n",
    "\n",
    "root = model.root\n",
    "print(\"Root\")\n",
    "print(f\"- split on: {index_to_name[root.split_on]}\" )\n",
    "print(f\"- children: {root.children}\", '\\n')\n",
    "\n",
    "pointy_node = root.children['Pointy']\n",
    "print(\"Pointy Node\")\n",
    "print(f\"- split on: {index_to_name[pointy_node.split_on]}\" )\n",
    "print(f\"- children: {pointy_node.children}\")\n",
    "print(f\"- data: \\n {pointy_node.data}\", '\\n')\n",
    "\n",
    "floppy_node = root.children['Floppy']\n",
    "print(\"Floppy Node\")\n",
    "print(f\"- split on: {index_to_name[floppy_node.split_on]}\" )\n",
    "print(f\"- children: {floppy_node.children}\")\n",
    "print(f\"- data: \\n {floppy_node.data}\", '\\n')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
